
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this 
                                                   %line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} 
                                 \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}


% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{LITERATURE REVIEW: \\
  Parallel Metropolis-Hastings Algorithms
}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Boyan Bejanov\\
{\em bbejanov@bankofcanada.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle



% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

The Monte Carlo methods (MC) are a class of numerical integration techniques,
where the integral is interpreted as the expected value of some function of
some random variable.  The expectation can be statistically estimated from a
sample drawn from the same distribution as the underlying random variable and
this estimate serves as the numerical approximation of the integral.  Needless
to say, the MC methods are also applicable to problems other than numerical
integrations, so long as they can be reduced to the evaluation of an expected
value.  

The successful application of MC relies on the availability of numerical
techniques for simulation of random samples from a distribution, which is given
by its probability density function (pdf).  We will denote this \emph{target}
density by $\pi(x)$.  We always assume that we have a stream of independent
random numbers with uniform distribution in the interval $(0,1)$.  For some of
the other standard probability distributions there are direct methods of
simulation, which perform some computation on the uniform random numbers, thus
transforming them into random numbers of the desired distribution.  However,
these techniques are limited to special cases.  One of the most important
general sampling technique is the Acceptance-Rejection sampling (AR).  Each
iteration of the AR goes in two steps.  We first simulate a random number, $X$,
from another distribution, which we can simulate by another known method.  We
call it \emph{proposal} density and denote by $q(x)$.  In the second step, we
simulate a uniform random number and compare it to the adjusted ratio of the
target and proposal densities.  Based on this comparison, we either accept $X$
as a random number from $\pi$, or we reject and discard it.  The AR method is
embarassingly parallel, and produces a sample of independednt random samples,
which can be used directly in an MC method.  However its applicability is
limited to the cases where $\pi(x)$ is given explicitly by a known formula.
Even then, the computational efficiency of AR (as measured by the acceptance
rate) depends on the choice of the proposal, $q(x)$, and a tight estimate of
the adjustment constant in the AR test, both of which can require some
non-trivial mathematical derivations, especially for multivatiate
distributions.  

The method of choice in the cases of a multivariate distribution of high
dimension, or when the target distribution formula is too complicated or can
only be computed numerically, is the Markov Chain Monte Carlo method (MCMC).  A
Markov Chain is a process, which is described by a \emph{state space}, i.e. the
set of all possible values the process can take, and a transition rule, which
may depend only on the current state, but not any earlier ones. It is known
from the theory of Markov chains that if certain conditions are satisfied, then
the process has an \emph{invariant distribution} and the distributions of
successive states converge to the invariant distribution.  In the MCMC method,
we construct a transition rule, such that the resulting Markov chain has an
invariant distribution, and this invariant distribution is exactly $\pi(x)$.
The initial part of the chain, before the distribution of states gets close
enough to the invariant distribution, is known as \emph{burn-in} and is
discarded.  The remaining chain can be used as the random sample in a Monte
Carlo method, however with the caveat that it is not an independent random
sample, so the auto-correlation of the process must to be taken into account.

The Metropolis-Hastings algorithm (M-H) combines AR and MCMC.  Effectively, it
is a recipe for constructing a Markov chain transition rule.  Once again we
have a proposal distribution, however this time the proposal distribution
depends on the current state of the chain, i.e. $q=q(x,y)$. The good news is
that the accept-reject test is a ratio of the values of $q$ and $\pi$ at the
current and the proposed points with no adjustment constants to be derived in
advance.  If the proposed point is accepted, then it becomes the next state of 
the chain, otherwise the next state is equal to the current state, i.e. no 
points are discarded as it was the case in the AR sampling method.

Other than Monte Carlo type estimations of expected values, the M-H is a
general method for simulating random samples from distributions that are
otherwise impossible to simulate.  It is particularly popular for Bayesian
analysis of complex stochastic models.  Such models always involve a number of
parameters, which rarely can be derived from first principles.  More
frequently, the paremeters are estimated from observed data using
maximum-likelihood, or some other methods of statistical estimation.  When the
model is sufficiently complex, the usual inference based on confidence
intervals for the estimated parameters is impossible to deriv.  In such cases,
the approach of Bayesian statistics is to consider the parameters as random
variables,  simulate a sample from their joint distribution, and use this
sample to infer the properties of the distribution that are of interest.

The specific focus in this project is the application of M-H to Bayesian
inference of large Dynamic Stochastic General Equilibrium models (DSGE), which
are used in the field of macro-economics to simulate the economy.  The
likelihood function is computed using a Kalman filter, which can be
computationally very expensive, especially in the nonlinear case.  Therefore,
when considering various methods, we will keep in mind the (simplifying)
assumprion that the evaluation of $\pi(x)$ is much more time consuming than all
the other computations in a single step (such as the computation of $q(x,y)$,
simulation from $q(x,y)$, individual arithmetic operations). 

The objectives of this project include
\begin{itemize}
\item Survey the existing methods and evaluate them.
\item Identify the most promising method, for the particular setting we have, and implement it.
\item Investigate the benefits of parallelization in terms of theoretical and
     practically achieved speedup.
\item Consider the question of P-completeness of the M-H algorithm.
\end{itemize}


% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################





Give an overview of the relevant literature. Cite all relevant
papers, like \cite{DEL07}, \cite{PD07}, \cite{DER07}, \cite{LDR07},
\cite{DLX06}, \cite{CDE06}, and \cite{DFL06}. Outline for each paper
the relevant results in relation to your project. Make sure that you
don't just list all relevant papers in random order. Devise a scheme
to group papers by subject. The goal is to present to the reader the
state-of-the-art in the field selected for your project.


% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
